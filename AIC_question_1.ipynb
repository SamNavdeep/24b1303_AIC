{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamNavdeep/24b1303_AIC/blob/main/AIC_question_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5seUkgSerDPd",
        "outputId": "45efe30e-3331-4c97-bb22-167a49007135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#=== import the libraries====\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5-8w4BkrxSS",
        "outputId": "42c4eecc-04d7-4a0e-9ae4-f1d1922304eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Accountant' 'Advocate' 'Agriculture' 'Apparel' 'Architecture' 'Arts'\n",
            " 'Automobile' 'Aviation' 'BPO' 'Banking' 'Blockchain'\n",
            " 'Building and Construction' 'Business Analyst' 'Civil Engineer'\n",
            " 'Consultant' 'Data Science' 'Database' 'Designing' 'DevOps'\n",
            " 'Digital Media' 'DotNet Developer' 'ETL Developer' 'Education'\n",
            " 'Electrical Engineering' 'Finance' 'Food and Beverages'\n",
            " 'Health and Fitness' 'Human Resources' 'Information Technology'\n",
            " 'Java Developer' 'Management' 'Mechanical Engineer'\n",
            " 'Network Security Engineer' 'Operations Manager' 'PMO' 'Public Relations'\n",
            " 'Python Developer' 'React Developer' 'SAP Developer' 'SQL Developer'\n",
            " 'Sales' 'Testing' 'Web Designing']\n"
          ]
        }
      ],
      "source": [
        "#====loading the data======\n",
        "from numpy import array\n",
        "csv_file_location='/content/train (2).csv'\n",
        "data=pd.read_csv(csv_file_location)\n",
        "category=data['Category']\n",
        "text=data['Text']\n",
        "category_arr= array(category)\n",
        "category_names=list(set(category_arr))\n",
        "category_names.sort()\n",
        "category_names=np.array(category_names)\n",
        "print(category_names)\n",
        "\n",
        "tokenize = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "token=[]\n",
        "stop_words=list(set((stopwords.words('english'))))\n",
        "stop_words_2=[]\n",
        "for words in stop_words:\n",
        "  upper_case=words.upper()\n",
        "  stop_words_2.append(upper_case)\n",
        "all_stop_words = stop_words + stop_words_2\n",
        "for n in range(len(text)):\n",
        "  token_array=word_tokenize(text[n])\n",
        "  token_array=[word for word in token_array if not word in all_stop_words]\n",
        "  token.append(token_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "H6PigMFRtxii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe8042e-3e77-4f78-a418-77c0bc8fffff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#===lemmatization========\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "\n",
        "\n",
        "tags=[]\n",
        "for n in range(len(token)):\n",
        "   tags.append(list(nltk.pos_tag(token[n])))\n",
        "\n",
        "def tagger(tag):\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "  elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "  elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "  else:\n",
        "        return None\n",
        "\n",
        "\n",
        "final_tag=[]\n",
        "for n in range(len(tags)):\n",
        " final_tag.append(list(map(lambda x :(x[0], tagger(x[1])), tags[n])))\n",
        "\n",
        "lemmatized_array=[]\n",
        "for n in range(len(final_tag)):\n",
        "  lemmatized_subarray=[]\n",
        "  for word, tag in final_tag[n]:\n",
        "    if tag is None:\n",
        "      lemmatized_subarray.append(word)\n",
        "    else:\n",
        "      lemmatized_subarray.append(lemmatizer.lemmatize(word, tag))\n",
        "  lemmatized_array.append(lemmatized_subarray)\n",
        "lemmatized_text=[]\n",
        "for subarray in lemmatized_array:\n",
        "  joined_text = \" \".join(subarray)\n",
        "  lemmatized_text.append(joined_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZqAXHwaKda1P"
      },
      "outputs": [],
      "source": [
        "#=====dividing the data into test and training sets=========\n",
        "import random\n",
        "random.seed(10)\n",
        "random.shuffle(lemmatized_text)\n",
        "random.seed(10)\n",
        "random.shuffle(category_arr)\n",
        "n=int((len(lemmatized_text))*0.8)\n",
        "training_data=lemmatized_text[:n]\n",
        "val_data=lemmatized_text[n:]\n",
        "training_labels=category_arr[:n]\n",
        "val_labels=category_arr[n:]\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "token_train=tokenizer(training_data, padding='max_length', max_length=512, truncation=True)\n",
        "token_val=tokenizer(val_data, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "train_id = torch.tensor(token_train['input_ids'])\n",
        "train_mask = torch.tensor(token_train['attention_mask'])\n",
        "val_id = torch.tensor(token_val['input_ids'])\n",
        "val_mask = torch.tensor(token_val['attention_mask'])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "train_labels = torch.tensor(encoder.fit_transform(training_labels))\n",
        "val_labels = torch.tensor(encoder.fit_transform(val_labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lpqJeBcQFnX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639d306d-114c-4e11-db38-251e3f90b032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size=32\n",
        "train_data = TensorDataset(train_id, train_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_data = TensorDataset(val_id, val_mask, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "model_0= BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=43)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YC6IQgaEmHrP"
      },
      "outputs": [],
      "source": [
        "# #============freezing========\n",
        "# for param in model_0.parameters():\n",
        "#   param.requires_grad=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YKl_62o65fk"
      },
      "outputs": [],
      "source": [
        "#============model_0 training============\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "optimizer = AdamW(model_0.parameters(),lr = 0.0001)\n",
        "\n",
        "cross_entropy  = nn.NLLLoss()\n",
        "train_acc, val_acc = [], []\n",
        "num_epochs = 5\n",
        "def evaluate(model,loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "       for steps, batch in enumerate(loader):\n",
        "          input_ids = batch[0]\n",
        "          attention_mask = batch[1]\n",
        "          labels = batch[2]\n",
        "\n",
        "          outputs = model_0(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          labels=labels)\n",
        "          logits=outputs.logits\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          total += batch.size(0)\n",
        "          correct += (preds == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_0.train()\n",
        "    for steps,batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0]\n",
        "        attention_mask = batch[1]\n",
        "        labels = batch[2]\n",
        "\n",
        "        outputs = model_0(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss = outputs.loss\n",
        "        logits=outputs.logits\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "train_acc = evaluate(model_0, train_dataloader)\n",
        "val_acc = evaluate(model_0, val_dataloader)\n",
        "train_acc.append(train_acc)\n",
        "val_acc.append(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eKV42-84cA-"
      },
      "outputs": [],
      "source": [
        "#==========fine tuning==============\n",
        "for param in model_0.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for epoch in range(3):\n",
        "    model_0.train()\n",
        "    for steps,batch in enumerate(train_dataloader):\n",
        "        outputs = model_0(\n",
        "        input_ids=batch['input_ids'],\n",
        "        attention_mask=batch['attention_mask'],\n",
        "        labels=batch['labels'])\n",
        "        optimizer.zero_grad()\n",
        "        loss = outputs.loss\n",
        "        logits=outputs.logits\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    val_acc=evaluate(model_0, val_dataloader)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1dTqIVvWJaBRVFvxVpV7hhhT4xK0UCPwT",
      "authorship_tag": "ABX9TyNWSLeh0E/4j03f6N1EsZaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}